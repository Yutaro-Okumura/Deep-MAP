{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title üî¨ Deep-MAP v1.0\n",
        "#@markdown **Welcome!** This notebook is a tool for automatically detecting and analyzing microplastics in microscope images.\n",
        "#@markdown ---\n",
        "#@markdown ### **How to Use**\n",
        "#@markdown 1.  **Prepare Your Files**: Prepare your image files in a ZIP archive.\n",
        "#@markdown     * **For a single area**: Place the images directly in the root of the ZIP file.\n",
        "#@markdown     * **For multiple areas**: Organize images into separate folders for each area, then add these folders to the ZIP archive (e.g., `Nagoya_River/`, `Indonesia_River/`).\n",
        "#@markdown 2.  **Run the Cells**: Execute the cells in order from top to bottom.\n",
        "#@markdown     * You can run all cells at once by selecting `Runtime` > `Run all`.\n",
        "#@markdown     * For faster processing, select `Runtime` > `Change runtime type` > `T4 GPU`.\n",
        "#@markdown 3.  **Input Information**: After uploading the file, fill in the form with the required analysis information.\n",
        "#@markdown 4.  **Download Results**: Once all processing is complete, the **analysis results in an Excel file** and the **annotated images** will be downloaded automatically."
      ],
      "metadata": {
        "id": "rLb3IIvmflyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Prepare Packages and Model\n",
        "#@markdown This cell installs required packages and downloads the pre-trained model.\n",
        "!pip install openpyxl ultralytics scikit-image ipywidgets -q\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "from tqdm.notebook import tqdm\n",
        "import glob\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "print(\"‚úÖ Packages are ready.\")\n",
        "# --- Model Download ---\n",
        "model_filename = \"best.pt\"\n",
        "file_id = \"1QC6I9zhcdSGjI6LWYTrbvkn-FyfxRyuk\"\n",
        "if not os.path.exists(model_filename):\n",
        "  print(\"Downloading model...\")\n",
        "  !gdown --id {file_id}\n",
        "  print(\"‚úÖ Model download complete.\")\n",
        "else:\n",
        "  print(\"‚úÖ Model already exists.\")"
      ],
      "metadata": {
        "id": "vn3_GP0tfv2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Upload Image ZIP File\n",
        "#@markdown Click the \"Choose Files\" button to upload the ZIP file containing your images.\n",
        "upload_dir = 'uploads'\n",
        "global analysis_root_dir\n",
        "analysis_root_dir = ''\n",
        "\n",
        "if os.path.exists(upload_dir): shutil.rmtree(upload_dir)\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "print('Please upload the ZIP file containing your images.')\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print('\\n‚ö†Ô∏è No file was uploaded.')\n",
        "else:\n",
        "    zip_filename = next(iter(uploaded))\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(upload_dir)\n",
        "    os.remove(zip_filename)\n",
        "    if os.path.exists(os.path.join(upload_dir, '__MACOSX')):\n",
        "        shutil.rmtree(os.path.join(upload_dir, '__MACOSX'))\n",
        "\n",
        "    # Automatically find the main data folder inside 'uploads'\n",
        "    extracted_items = os.listdir(upload_dir)\n",
        "    if len(extracted_items) == 1 and os.path.isdir(os.path.join(upload_dir, extracted_items[0])):\n",
        "        analysis_root_dir = os.path.join(upload_dir, extracted_items[0])\n",
        "        print(f\"\\n‚úÖ Files extracted. Main data folder found at: '{analysis_root_dir}'\")\n",
        "    else:\n",
        "        analysis_root_dir = upload_dir\n",
        "        print(f'\\n‚úÖ Files extracted and ready in the \"{analysis_root_dir}\" directory.')"
      ],
      "metadata": {
        "id": "Hbzn-tmxf2TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Input Analysis Information\n",
        "#@markdown Please enter the information for each detected sample area.\n",
        "#@markdown **Sample collection areas (subdirectories) are detected automatically.**\n",
        "\n",
        "# --- Auto-detect subdirectories (sample areas) from the correct root folder ---\n",
        "try:\n",
        "    if not analysis_root_dir or not os.path.exists(analysis_root_dir):\n",
        "         print(\"‚ö†Ô∏è Error: Analysis root directory not found. Please run Cell 2 again to upload files.\")\n",
        "    else:\n",
        "        sub_dirs = [d for d in os.listdir(analysis_root_dir) if os.path.isdir(os.path.join(analysis_root_dir, d))]\n",
        "        all_images_in_root = glob.glob(os.path.join(analysis_root_dir, '*.[pP][nN][gG]')) + \\\n",
        "                             glob.glob(os.path.join(analysis_root_dir, '*.[jJ][pP]*[gG]'))\n",
        "        if not sub_dirs and all_images_in_root:\n",
        "            sub_dirs = ['Default_Area']\n",
        "        print(f\"Detected sample areas: {', '.join(sub_dirs)}\\n\")\n",
        "\n",
        "        # --- Create input forms for each area ---\n",
        "        style = {'description_width': '150px'}\n",
        "        layout = widgets.Layout(width='400px')\n",
        "        area_widgets = {}\n",
        "        all_forms = []\n",
        "\n",
        "        for area in sub_dirs:\n",
        "            cat_widget = widgets.Text(description=f\"Categories (e.g., River):\", style=style, layout=layout)\n",
        "            lat_widget = widgets.Text(description=f\"Latitude:\", style=style, layout=layout)\n",
        "            lon_widget = widgets.Text(description=f\"Longitude:\", style=style, layout=layout)\n",
        "            volume_widget = widgets.FloatText(description=f\"Sample Volume (L):\", value=1.0, style=style, layout=layout)\n",
        "\n",
        "            area_widgets[area] = {\n",
        "                'categories': cat_widget,\n",
        "                'latitude': lat_widget,\n",
        "                'longitude': lon_widget,\n",
        "                'sample_volume': volume_widget\n",
        "            }\n",
        "\n",
        "            area_form = widgets.VBox([\n",
        "                widgets.HTML(f\"<b>Information for Area: {area}</b>\"),\n",
        "                cat_widget,\n",
        "                lat_widget,\n",
        "                lon_widget,\n",
        "                volume_widget\n",
        "            ])\n",
        "            all_forms.append(area_form)\n",
        "\n",
        "        # --- Global settings ---\n",
        "        global_settings_form = widgets.VBox([\n",
        "            widgets.HTML(f\"<hr><b>Global Settings (for all areas)</b>\"),\n",
        "            widgets.FloatText(description=\"Scale: 1 pixel = ? Œºm:\", value=1.0, style=style, layout=layout),\n",
        "            widgets.FloatSlider(description=\"Confidence Threshold:\", min=0.05, max=1.0, step=0.05, value=0.25, style=style, layout=layout)\n",
        "        ])\n",
        "        scale_widget = global_settings_form.children[1]\n",
        "        conf_widget = global_settings_form.children[2]\n",
        "        all_forms.append(global_settings_form)\n",
        "\n",
        "        # --- Button and submission ---\n",
        "        button = widgets.Button(description=\"Confirm All Information and Start Analysis\", button_style='success', layout=widgets.Layout(width='90%'))\n",
        "        global metadata\n",
        "        metadata = {'areas': {}}\n",
        "\n",
        "        def on_button_click(b):\n",
        "            for area in sub_dirs:\n",
        "                metadata['areas'][area] = {\n",
        "                    'categories': area_widgets[area]['categories'].value, # ‚òÖ‚òÖ‚òÖ Get value\n",
        "                    'latitude': area_widgets[area]['latitude'].value,\n",
        "                    'longitude': area_widgets[area]['longitude'].value,\n",
        "                    'sample_volume': area_widgets[area]['sample_volume'].value if area_widgets[area]['sample_volume'].value > 0 else None,\n",
        "                }\n",
        "            metadata['scale'] = scale_widget.value if scale_widget.value > 0 else None\n",
        "            metadata['conf'] = conf_widget.value\n",
        "\n",
        "            clear_output(wait=True)\n",
        "            print(\"‚úÖ Starting analysis with the following information:\")\n",
        "            print(f\"  - Global Scale: {metadata['scale']}\")\n",
        "            print(f\"  - Global Confidence: {metadata['conf']}\")\n",
        "            for area, data in metadata['areas'].items():\n",
        "                print(f\"  --- Area: {area} ---\")\n",
        "                for k, v in data.items():\n",
        "                    print(f\"    - {k}: {v}\")\n",
        "            run_analysis_and_export()\n",
        "\n",
        "        button.on_click(on_button_click)\n",
        "        display(widgets.VBox(all_forms + [button]))\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è Error: `analysis_root_dir` is not defined. Please run Cell 2 again to upload files.\")"
      ],
      "metadata": {
        "id": "QnmPgknpf6oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Run Analysis and Export Results\n",
        "#@markdown This cell will perform the analysis on all your images and automatically download the results.\n",
        "\n",
        "def run_analysis_and_export():\n",
        "    from ultralytics.utils.plotting import Annotator, colors\n",
        "    def get_color_advanced(image, mask, k=3, s_thresh=50, v_thresh=50, fallback_threshold=0.1):\n",
        "        # 1. Get all pixels within the mask first\n",
        "        all_pixels = image[np.where(mask > 0)]\n",
        "        if len(all_pixels) == 0:\n",
        "            return 'Unknown'\n",
        "\n",
        "        # 2. Try to extract vibrant pixels\n",
        "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        vibrant_mask = cv2.bitwise_and(\n",
        "            cv2.inRange(hsv_image[:,:,1], s_thresh, 255),\n",
        "            cv2.inRange(hsv_image[:,:,2], v_thresh, 255)\n",
        "        )\n",
        "        final_mask = cv2.bitwise_and(mask, vibrant_mask)\n",
        "        vibrant_pixels = image[np.where(final_mask > 0)]\n",
        "\n",
        "        # 3. Check the ratio of vibrant pixels\n",
        "        if len(vibrant_pixels) < (len(all_pixels) * fallback_threshold):\n",
        "            mean_bgr = np.mean(all_pixels, axis=0)\n",
        "        else:\n",
        "            if len(vibrant_pixels) < k:\n",
        "                mean_bgr = np.mean(vibrant_pixels, axis=0)\n",
        "            else:\n",
        "                pixels = np.float32(vibrant_pixels)\n",
        "                criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "                _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "                _, counts = np.unique(labels, return_counts=True)\n",
        "                mean_bgr = centers[np.argmax(counts)]\n",
        "\n",
        "        # 4. Determine color name by comparing the representative color to reference colors\n",
        "        color_references = {\n",
        "            'black': [25, 25, 25], 'white': [230, 230, 230], 'red': [0, 0, 200],\n",
        "            'green': [0, 180, 0], 'blue': [200, 0, 0]\n",
        "        }\n",
        "        min_dist, detected_color = float('inf'), 'Unknown'\n",
        "        for color_name, bgr_value in color_references.items():\n",
        "            dist = np.linalg.norm(mean_bgr - np.array(bgr_value))\n",
        "            if dist < min_dist:\n",
        "                min_dist, detected_color = dist, color_name\n",
        "\n",
        "        return detected_color\n",
        "\n",
        "    try:\n",
        "        model = YOLO(model_filename)\n",
        "        print(\"\\n‚úÖ Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to load model: {e}\")\n",
        "        return\n",
        "\n",
        "    all_results = []\n",
        "    conf_threshold = metadata.get('conf', 0.25)\n",
        "    print(f\"Confidence threshold for analysis: {conf_threshold}\")\n",
        "\n",
        "    results_img_dir = 'results_images'\n",
        "    if os.path.exists(results_img_dir): shutil.rmtree(results_img_dir)\n",
        "    os.makedirs(results_img_dir)\n",
        "\n",
        "    particle_id_counter = 1\n",
        "\n",
        "    for area_name in sub_dirs:\n",
        "        print(f\"\\n--- Processing images for area: '{area_name}' ---\")\n",
        "        current_image_dir = os.path.join(analysis_root_dir, area_name) if area_name != 'Default_Area' else analysis_root_dir\n",
        "        area_results_dir = os.path.join(results_img_dir, area_name)\n",
        "        os.makedirs(area_results_dir, exist_ok=True)\n",
        "\n",
        "        image_files = glob.glob(os.path.join(current_image_dir, '*.[pP][nN][gG]')) + glob.glob(os.path.join(current_image_dir, '*.[jJ][pP]*[gG]'))\n",
        "\n",
        "        for image_path in tqdm(image_files, desc=f\"Processing {area_name}\"):\n",
        "            img = cv2.imread(image_path)\n",
        "            if img is None: continue\n",
        "\n",
        "            results = model(image_path, conf=conf_threshold, verbose=False)[0]\n",
        "\n",
        "            annotated_img = results.plot(labels=False, conf=False)\n",
        "\n",
        "            if results.masks is not None:\n",
        "                for i in range(len(results.boxes)):\n",
        "                    box = results.boxes[i]\n",
        "                    cls_idx = int(box.cls)\n",
        "                    class_name = model.names[cls_idx]\n",
        "                    confidence = float(box.conf)\n",
        "\n",
        "                    label = f\"ID:{particle_id_counter} {class_name} {confidence:.2f}\"\n",
        "\n",
        "                    xyxy = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                    x1, y1, x2, y2 = xyxy\n",
        "\n",
        "                    font_scale = 1.0\n",
        "                    font_thickness = 2\n",
        "\n",
        "                    (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n",
        "                    cv2.rectangle(annotated_img, (x1, y1), (x1 + text_width, y1 - text_height - baseline), colors(cls_idx, True), -1)\n",
        "                    cv2.putText(annotated_img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)\n",
        "\n",
        "                    mask_data = results.masks.data[i]\n",
        "                    mask = cv2.resize(mask_data.cpu().numpy(), (img.shape[1], img.shape[0])).astype(np.uint8)\n",
        "                    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                    all_results.append({\n",
        "                        'particle_id': particle_id_counter,\n",
        "                        'area_name': area_name, 'image_name': os.path.basename(image_path),\n",
        "                        'class': class_name,\n",
        "                        'confidence': confidence,\n",
        "                        'color': get_color_advanced(img, mask),\n",
        "                        'area': np.sum(mask),\n",
        "                        'width': cv2.fitEllipse(contours[0])[1][0] if contours and len(contours[0]) >= 5 else 0\n",
        "                    })\n",
        "\n",
        "                    particle_id_counter += 1\n",
        "\n",
        "            cv2.imwrite(os.path.join(area_results_dir, f\"result_{os.path.basename(image_path)}\"), annotated_img)\n",
        "\n",
        "    if not all_results:\n",
        "        print(\"\\n‚ö†Ô∏è No microplastics were detected in the images.\")\n",
        "    else:\n",
        "        df_all = pd.DataFrame(all_results)\n",
        "        scale = metadata.get('scale')\n",
        "        len_unit = 'um' if scale else 'px'\n",
        "        area_unit = 'um2' if scale else 'px2'\n",
        "\n",
        "        df_all[f'width_{len_unit}'] = df_all['width'] * scale if scale else df_all['width']\n",
        "        df_all[f'area_{area_unit}'] = df_all['area'] * (scale**2) if scale else df_all['area']\n",
        "\n",
        "        OUTPUT_EXCEL_PATH = f\"analysis_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
        "        with pd.ExcelWriter(OUTPUT_EXCEL_PATH) as writer:\n",
        "            all_classes = ['Fragment', 'Fiber', 'Foam', 'Pellet', 'Film']\n",
        "            all_colors = ['black', 'red', 'green', 'blue', 'white']\n",
        "\n",
        "            for area_name in df_all['area_name'].unique():\n",
        "                df_area = df_all[df_all['area_name'] == area_name].copy()\n",
        "\n",
        "                df_area_raw = df_area[['particle_id', 'image_name', 'class', 'confidence', 'color', f'area_{area_unit}', f'width_{len_unit}']]\n",
        "                df_area_raw.to_excel(writer, sheet_name=f\"{area_name}_raw_data\", index=False)\n",
        "\n",
        "                count_pivot = df_area.pivot_table(index='class', columns='color', values='image_name', aggfunc='size', fill_value=0)\n",
        "                width_pivot = df_area.pivot_table(index='class', columns='color', values=f'width_{len_unit}', aggfunc='mean', fill_value=0)\n",
        "                area_pivot = df_area.pivot_table(index='class', columns='color', values=f'area_{area_unit}', aggfunc='mean', fill_value=0)\n",
        "                count_pivot = count_pivot.reindex(index=all_classes, columns=all_colors, fill_value=0); width_pivot = width_pivot.reindex(index=all_classes, columns=all_colors, fill_value=0); area_pivot = area_pivot.reindex(index=all_classes, columns=all_colors, fill_value=0)\n",
        "                new_idx = pd.MultiIndex.from_tuples([(c, m) for c in all_classes for m in ['number', 'average_width', 'average_area']], names=['class', 'metric'])\n",
        "                summary_df = pd.DataFrame(index=new_idx, columns=all_colors)\n",
        "                for c in all_classes:\n",
        "                    summary_df.loc[(c, 'number'), :] = count_pivot.loc[c, :]; summary_df.loc[(c, 'average_width'), :] = width_pivot.loc[c, :]; summary_df.loc[(c, 'average_area'), :] = area_pivot.loc[c, :]\n",
        "                summary_df.to_excel(writer, sheet_name=f\"{area_name}_summary\")\n",
        "\n",
        "            summary_list = []\n",
        "            for area_name in df_all['area_name'].unique():\n",
        "                df_area = df_all[df_all['area_name'] == area_name]\n",
        "                total_count = len(df_area)\n",
        "                area_meta = metadata['areas'].get(area_name, {})\n",
        "                sample_volume = area_meta.get('sample_volume')\n",
        "                categories = area_meta.get('categories', '')\n",
        "\n",
        "                if sample_volume and sample_volume > 0: abundance = total_count / sample_volume\n",
        "                else: abundance = \"N/A (Volume not provided)\"\n",
        "\n",
        "                smp_countA = df_area[df_area[f'width_{len_unit}'].between(1, 100)].shape[0] if scale else \"N/A\"\n",
        "                smp_countB = df_area[df_area[f'width_{len_unit}'].between(100, 300)].shape[0] if scale else \"N/A\"\n",
        "                smp_countC = df_area[df_area[f'width_{len_unit}'].between(300, 600)].shape[0] if scale else \"N/A\"\n",
        "                smp_countD = df_area[df_area[f'width_{len_unit}'].between(600, 999.99)].shape[0] if scale else \"N/A\"\n",
        "                lmp_count = df_area[df_area[f'width_{len_unit}'].between(1000, 5000)].shape[0] if scale else \"N/A\"\n",
        "                shape_pct = (df_area['class'].value_counts(normalize=True) * 100).to_dict()\n",
        "                color_pct = (df_area['color'].value_counts(normalize=True) * 100).to_dict()\n",
        "\n",
        "                summary_list.append({\n",
        "                    'Location': area_name,\n",
        "                    'Abundance (particles/L)': f\"{abundance:.2f}\" if isinstance(abundance, (int, float)) else abundance,\n",
        "                    'Categories': categories,\n",
        "                    'Latitude': area_meta.get('latitude', ''), 'Longitude': area_meta.get('longitude', ''),\n",
        "                    'Sample Volume (L)': sample_volume if sample_volume else \"N/A\",\n",
        "                    'Dominant Shape': df_area['class'].mode()[0] if not df_area.empty else \"N/A\",\n",
        "                    'Fragment':f\"{shape_pct.get('Fragment',0):.0f}%\", 'Fiber':f\"{shape_pct.get('Fiber',0):.0f}%\",\n",
        "                    'Pellet':f\"{shape_pct.get('Pellet',0):.0f}%\", 'Foam':f\"{shape_pct.get('Foam',0):.0f}%\", 'Film':f\"{shape_pct.get('Film',0):.0f}%\",\n",
        "                    'Dominant Colour': df_area['color'].mode()[0] if not df_area.empty else \"N/A\",\n",
        "                    'Black':f\"{color_pct.get('black',0):.0f}%\", 'Red':f\"{color_pct.get('red',0):.0f}%\",\n",
        "                    'Green':f\"{color_pct.get('green',0):.0f}%\", 'Blue':f\"{color_pct.get('blue',0):.0f}%\", 'White':f\"{color_pct.get('white',0):.0f}%\",\n",
        "                    'SMP (1-100um)': f\"{smp_countA/total_count*100:.0f}%\" if scale and total_count>0 else \"N/A\",\n",
        "                    'SMP (100-300um)': f\"{smp_countB/total_count*100:.0f}%\" if scale and total_count>0 else \"N/A\",\n",
        "                    'SMP (300-600um)': f\"{smp_countC/total_count*100:.0f}%\" if scale and total_count>0 else \"N/A\",\n",
        "                    'SMP (600-1000um)': f\"{smp_countD/total_count*100:.0f}%\" if scale and total_count>0 else \"N/A\",\n",
        "                    'LMP (1000-5000um)': f\"{lmp_count/total_count*100:.0f}%\" if scale and total_count>0 else \"N/A\"\n",
        "                })\n",
        "\n",
        "            df_summary_final = pd.DataFrame(summary_list)\n",
        "            desired_order = ['Location', 'Abundance (particles/L)', 'Latitude', 'Longitude', 'Sample Volume (L)', 'Categories', 'Dominant Shape', 'Fragment', 'Fiber', 'Pellet', 'Foam', 'Film', 'Dominant Colour', 'Black', 'Red', 'Green', 'Blue', 'White', 'SMP (1-100um)','SMP (100-300um)', 'SMP (300-600um)', 'SMP (600-1000um)', 'LMP (1000-5000um)']\n",
        "            df_summary_final = df_summary_final.reindex(columns=desired_order)\n",
        "            df_summary_final.to_excel(writer, sheet_name='All_Summary', index=False)\n",
        "\n",
        "        print(f\"\\n‚úÖ Analysis complete. Results saved to {OUTPUT_EXCEL_PATH}\")\n",
        "        files.download(OUTPUT_EXCEL_PATH)\n",
        "        shutil.make_archive('results_images', 'zip', results_img_dir)\n",
        "        print(f\"\\n‚úÖ Annotated images saved to results_images.zip\")\n",
        "        files.download('results_images.zip')"
      ],
      "metadata": {
        "id": "srB1B3Z_gFuE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}